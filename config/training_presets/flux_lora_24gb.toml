# Kohya FLUX LoRA Training - 24GB VRAM Preset
# Optimized for RTX 4090, RTX 3090, A5000 (24GB+)
# Uses AdamW8bit optimizer with higher resolution

[preset_info]
name = "24GB VRAM"
description = "High-quality preset for 24GB+ GPUs"
target_gpus = ["RTX 4090", "RTX 3090", "A5000", "A6000"]
estimated_vram_peak = "20-22GB"

[model_arguments]
# Model paths are injected at runtime by KohyaTrainerService
pretrained_model_name_or_path = "{model_path}"
ae = "{vae_path}"
clip_l = "{clip_l_path}"
t5xxl = "{t5xxl_path}"

[additional_network_arguments]
network_module = "networks.lora_flux"
network_dim = 32
network_alpha = 16

[optimizer_arguments]
optimizer_type = "adamw8bit"
learning_rate = 0.0001

[training_arguments]
output_dir = "{output_dir}"
output_name = "cg_{character_name}"
save_model_as = "safetensors"
max_train_steps = 2000
mixed_precision = "bf16"
gradient_checkpointing = true
gradient_accumulation_steps = 2
seed = 42
save_every_n_steps = 500
cache_latents = true
cache_latents_to_disk = true
cache_text_encoder_outputs = true
cache_text_encoder_outputs_to_disk = true

[flux_specific_arguments]
blocks_to_swap = 0
t5xxl_max_token_length = 512
guidance_scale = 1.0

[dataset_arguments]
resolution = 768
enable_bucket = true
bucket_no_upscale = true

[[datasets]]
batch_size = 1

[[datasets.subsets]]
image_dir = "{images_dir}"
caption_extension = ".txt"
num_repeats = 10
